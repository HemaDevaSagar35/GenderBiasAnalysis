{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import analysis_util as au\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys, os\n",
    "from glob import glob\n",
    "import pickle5 as pickle\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(\"/localdata2/jent_so/LM_GenderBias\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posneg(list):\n",
    "    pos = [x for x in list if x>0]\n",
    "    neg = [x for x in list if x<0]\n",
    "    \n",
    "#    print(len(pos), len(neg)) \n",
    "    return np.mean(pos), np.mean(neg), len(pos), len(neg)\n",
    "    \n",
    "\n",
    "def rename(name):\n",
    "    specs = [\n",
    "        ['N_pro', 'remove Pronouns'], \n",
    "        ['N_weat', 'remove WEAT'], \n",
    "        ['N_all', 'remove All'],\n",
    "        ['mix_pro', 'mix Pronouns'], \n",
    "        ['mix_weat', 'mix WEAT'], \n",
    "        ['mix_all', 'mix All'], \n",
    "        ['original_Rall', 'All'], \n",
    "        ['original_Rweat', 'WEAT'], \n",
    "        ['original_Rpro', 'Pronouns'], \n",
    "    ]\n",
    "        #['original']\n",
    "    for spec in specs:\n",
    "        if spec[0] in name:\n",
    "            return spec[0]\n",
    "        #    return spec[1]\n",
    "    print(\"error\")\n",
    "\n",
    "\n",
    "def calc_bias_dict(df_dict):\n",
    "    bias_dict = {}\n",
    "\n",
    "    for spec in df_dict.keys():\n",
    "        bias_l = df_dict[spec].bias.tolist()\n",
    "        # total bias\n",
    "        overall_bias_total = np.mean(bias_l)\n",
    "        overall_bias_total_noZero = np.mean([i for i in bias_l if i != 0])\n",
    "        # absolute bias\n",
    "        overall_bias_abs = np.mean([abs(x) for x in bias_l])\n",
    "        overall_bias_abs_noZero = np.mean([abs(x) for x in bias_l if x != 0])\n",
    "        # pos neg bias \n",
    "        pos, neg, pos_n, neg_n = posneg(bias_l) \n",
    "\n",
    "        bias_dict[spec] = [\n",
    "            overall_bias_total, # 0\n",
    "            overall_bias_abs, # 1\n",
    "            pos, neg, # 2 3 \n",
    "            pos_n, neg_n, # 4 5 \n",
    "            overall_bias_total_noZero, # 6\n",
    "            overall_bias_abs_noZero  ] # 7 \n",
    "    return bias_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bias(task = 'IMDB', model_id_= None):\n",
    "    files = glob(\"../res_results/ratings/*\")\n",
    "    o = '../res_results/ratings/rating_{}_{}_original'.format(task, model_id_)\n",
    "    if o in files:\n",
    "        files.remove(o)\n",
    "\n",
    "    df_dict = {}\n",
    "    for file in files: \n",
    "        if '_{}_'.format(model_id_) in file and task in file:\n",
    "            with open (file, \"rb\") as fh:\n",
    "                data = pickle.load(fh)\n",
    "            df_dict[rename(file)] = data\n",
    "    return df_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMDB_training_details = [\n",
    "    [\"IMDB\", \"albertbase\", \"N_all\", 0.1, 5 ],  \n",
    "    [\"IMDB\", \"albertbase\", \"N_pro\", 0.05, 4 ], \n",
    "    [\"IMDB\", \"albertbase\", \"N_weat\", 0.05, 8 ],\n",
    "    [\"IMDB\", \"albertbase\", \"mix_all\", 0.1, 19 ],\n",
    "    [\"IMDB\", \"albertbase\", \"mix_pro\", 0.1, 13 ], \n",
    "    [\"IMDB\", \"albertbase\", \"mix_weat\", 0.2, 6 ],\n",
    "    [\"IMDB\", \"albertbase\", \"original\", 0.1, 8 ], \n",
    "    [\"IMDB\", \"albertlarge\", \"N_all\", 0.05, 17 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"N_pro\", 0.05, 11 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"N_weat\", 0.05, 12 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"mix_all\", 0.2, 18 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"mix_pro\", 0.1, 7 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"mix_weat\", 0.1, 19 ],\n",
    "    [\"IMDB\", \"albertlarge\", \"original\", 0.2, 12 ], \n",
    "    [\"IMDB\", \"bertbase\", \"N_all\", 0.1, 12 ],\n",
    "    [\"IMDB\", \"bertbase\", \"N_pro\", 0.1, 11 ],\n",
    "    [\"IMDB\", \"bertbase\", \"N_weat\", 0.1, 12 ],\n",
    "    [\"IMDB\", \"bertbase\", \"mix_all\", 0.2, 12], \n",
    "    [\"IMDB\", \"bertbase\", \"mix_pro\", 0.2, 5 ], \n",
    "    [\"IMDB\", \"bertbase\", \"mix_weat\", 0.2, 10 ],\n",
    "    [\"IMDB\", \"bertbase\", \"original\", 0.1, 10 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"N_all\", 0.05, 19 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"N_pro\", 0.05, 7 ], \n",
    "    [\"IMDB\", \"bertlarge\", \"N_weat\", 0.1, 6 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"mix_all\", 0.2, 14 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"mix_pro\", 0.2, 19 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"mix_weat\", 0.2, 13 ],\n",
    "    [\"IMDB\", \"bertlarge\", \"original\", 0.05, 13],\n",
    "    [\"IMDB\", \"distbase\", \"N_all\", 0.05, 16 ],\n",
    "    [\"IMDB\", \"distbase\", \"N_pro\", 0.05, 18 ],\n",
    "    [\"IMDB\", \"distbase\", \"N_weat\", 0.05, 19 ],\n",
    "    [\"IMDB\", \"distbase\", \"mix_all\", 0.2, 14 ],\n",
    "    [\"IMDB\", \"distbase\", \"mix_pro\", 0.2, 18 ],\n",
    "    [\"IMDB\", \"distbase\", \"mix_weat\", 0.2, 19 ],\n",
    "    [\"IMDB\", \"distbase\", \"original\", 0.05, 8 ],\n",
    "    [\"IMDB\", \"robertabase\", \"N_all\", 0.05, 10],\n",
    "    [\"IMDB\", \"robertabase\", \"N_pro\", 0.05, 7],\n",
    "    [\"IMDB\", \"robertabase\", \"N_weat\", 0.05, 10],\n",
    "    [\"IMDB\", \"robertabase\", \"mix_all\", 0.05, 11],\n",
    "    [\"IMDB\", \"robertabase\", \"mix_pro\", 0.05, 4],\n",
    "    [\"IMDB\", \"robertabase\", \"mix_weat\", 0.05, 7],\n",
    "    [\"IMDB\", \"robertabase\", \"original\", 0.05, 8],\n",
    "    [\"IMDB\", \"robertalarge\", \"N_all\", 0.05, 15 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"N_pro\", 0.05, 10 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"N_weat\", 0.05, 14 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"mix_all\", 0.05, 5 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"mix_pro\", 0.05, 5 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"mix_weat\", 0.05, 5 ],\n",
    "    [\"IMDB\", \"robertalarge\", \"original\", 0.05, 11 ]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r(n, d = 4):\n",
    "    return round(n, d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "---  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albertbase &  original_Rall &  -0.0019348283542320133 &  -0.00233033236285592 &  9112 &  11645 &  4243 \\\\ \n",
      "albertbase &  original_Rweat &  0.00014532426968216896 &  0.00019842199574299422 &  7710 &  10600 &  6690 \\\\ \n",
      "albertbase &  original_Rpro &  0.0007203491327539087 &  0.0011053724723083548 &  5481 &  10811 &  8708 \\\\ \n",
      "albertbase &  N_pro &  -0.00028908527866005896 &  -0.0004436272768093717 &  9305 &  6986 &  8709 \\\\ \n",
      "albertbase &  N_weat &  -0.0032442289778590204 &  -0.004434914941298967 &  10346 &  7942 &  6712 \\\\ \n",
      "albertbase &  N_all &  0.0007778982762806117 &  0.0009373171827171435 &  8979 &  11769 &  4252 \\\\ \n",
      "albertbase &  mix_pro &  0.0013528452794149051 &  0.0020861788789398364 &  7244 &  8968 &  8788 \\\\ \n",
      "albertbase &  mix_weat &  -0.002433704402707517 &  -0.003439573184899538 &  9426 &  8263 &  7311 \\\\ \n",
      "albertbase &  mix_all &  -0.0011561673726711889 &  -0.0014092040523026531 &  9481 &  11030 &  4489 \\\\ \n",
      "albertlarge &  original_Rall &  0.011371183886015789 &  0.013740615648431277 &  5095 &  15594 &  4311 \\\\ \n",
      "albertlarge &  original_Rweat &  0.009515320429047569 &  0.013035399787724765 &  4058 &  14191 &  6751 \\\\ \n",
      "albertlarge &  original_Rpro &  0.005565414380179718 &  0.008591254060172459 &  2120 &  14075 &  8805 \\\\ \n",
      "albertlarge &  N_pro &  0.002235921167731285 &  0.0034343837056575403 &  6407 &  9869 &  8724 \\\\ \n",
      "albertlarge &  N_weat &  -0.0023230286708753557 &  -0.003171976447205412 &  9936 &  8373 &  6691 \\\\ \n",
      "albertlarge &  N_all &  -0.0026336579717043786 &  -0.0031726232011087298 &  12573 &  8180 &  4247 \\\\ \n",
      "albertlarge &  mix_pro &  -0.0004892830661474727 &  -0.0007524190597088527 &  10121 &  6136 &  8743 \\\\ \n",
      "albertlarge &  mix_weat &  -0.0006201677279701108 &  -0.0008526283105616348 &  9186 &  8998 &  6816 \\\\ \n",
      "albertlarge &  mix_all &  0.0028237285806685395 &  0.003418226540611732 &  8777 &  11875 &  4348 \\\\ \n",
      "bertbase &  original_Rall &  0.0029154255445068703 &  0.003510868911978408 &  5233 &  15527 &  4240 \\\\ \n",
      "bertbase &  original_Rweat &  0.00113096886331914 &  0.0015437740422046683 &  6187 &  12128 &  6685 \\\\ \n",
      "bertbase &  original_Rpro &  0.0008250461028167047 &  0.0012650976797361148 &  5430 &  10874 &  8696 \\\\ \n",
      "bertbase &  N_pro &  0.002023557659597136 &  0.0031045683639109177 &  3234 &  13061 &  8705 \\\\ \n",
      "bertbase &  N_weat &  0.001472137361841742 &  0.002010896844390971 &  6204 &  12098 &  6698 \\\\ \n",
      "bertbase &  N_all &  0.0034085800484195353 &  0.004106722949903055 &  4319 &  16431 &  4250 \\\\ \n",
      "bertbase &  mix_pro &  -2.0187031316972936e-05 &  -3.298750133501035e-05 &  7505 &  7794 &  9701 \\\\ \n",
      "bertbase &  mix_weat &  -0.00012100960539975858 &  -0.00022316613565904136 &  6421 &  7135 &  11444 \\\\ \n",
      "bertbase &  mix_all &  0.00030574339576195823 &  0.00048258001730216277 &  6838 &  9001 &  9161 \\\\ \n",
      "bertlarge &  original_Rall &  0.0007077636280376464 &  0.0008522754540215385 &  9128 &  11633 &  4239 \\\\ \n",
      "bertlarge &  original_Rweat &  -0.002319743663556874 &  -0.0031664532672084 &  10329 &  7986 &  6685 \\\\ \n",
      "bertlarge &  original_Rpro &  -0.0010644473167182877 &  -0.0016318870986666582 &  10287 &  6020 &  8693 \\\\ \n",
      "bertlarge &  N_pro &  0.0030112049669749103 &  0.0046164300100798895 &  2697 &  13610 &  8693 \\\\ \n",
      "bertlarge &  N_weat &  -0.000839984898394905 &  -0.001146643139667611 &  10172 &  8142 &  6686 \\\\ \n",
      "bertlarge &  N_all &  0.003495154024688527 &  0.0042096088364028126 &  7314 &  13443 &  4243 \\\\ \n",
      "bertlarge &  mix_pro &  0.00043477316963265705 &  0.0010667709530686454 &  4961 &  5228 &  14811 \\\\ \n",
      "bertlarge &  mix_weat &  0.0017597068528624163 &  0.0034433837916061686 &  4581 &  8195 &  12224 \\\\ \n",
      "bertlarge &  mix_all &  0.001157470835662407 &  0.0014990815361114942 &  8848 &  10455 &  5697 \\\\ \n",
      "distbase &  original_Rall &  0.0012895546359568833 &  0.0015530815058734987 &  7817 &  12941 &  4242 \\\\ \n",
      "distbase &  original_Rweat &  0.00029589569197036324 &  0.0004039641928385256 &  8098 &  10214 &  6688 \\\\ \n",
      "distbase &  original_Rpro &  0.0006152340499591082 &  0.0009435526194084845 &  6085 &  10216 &  8699 \\\\ \n",
      "distbase &  N_pro &  0.0006730709419678897 &  0.0010323807318974932 &  7116 &  9183 &  8701 \\\\ \n",
      "distbase &  N_weat &  -0.0010898587383935227 &  -0.0014884713717475044 &  10773 &  7532 &  6695 \\\\ \n",
      "distbase &  N_all &  0.0006920536209689453 &  0.0008335585143680686 &  10022 &  10734 &  4244 \\\\ \n",
      "distbase &  mix_pro &  -0.0006946435683396339 &  -0.0012202999935697315 &  6922 &  7309 &  10769 \\\\ \n",
      "distbase &  mix_weat &  -0.0005545579573206123 &  -0.0008272045902753764 &  8332 &  8428 &  8240 \\\\ \n",
      "distbase &  mix_all &  -0.0002555812667540522 &  -0.00031542339284451327 &  10080 &  10177 &  4743 \\\\ \n",
      "robertabase &  original_Rall &  0.0016281069995695726 &  0.0019615746982765934 &  7165 &  13585 &  4250 \\\\ \n",
      "robertabase &  original_Rweat &  0.0011428216321393847 &  0.0015610611301215506 &  6470 &  11832 &  6698 \\\\ \n",
      "robertabase &  original_Rpro &  0.0010234887690283357 &  0.0015709245595351422 &  5448 &  10840 &  8712 \\\\ \n",
      "robertabase &  N_pro &  0.0005955476485867985 &  0.0009137529897305733 &  6822 &  9472 &  8706 \\\\ \n",
      "robertabase &  N_weat &  0.00048557181512471286 &  0.0006632407462229046 &  7722 &  10581 &  6697 \\\\ \n",
      "robertabase &  N_all &  0.0008268527296558022 &  0.0009958241758066797 &  9294 &  11464 &  4242 \\\\ \n",
      "robertabase &  mix_pro &  -0.00011369902121368796 &  -0.00017444921629693132 &  8682 &  7612 &  8706 \\\\ \n",
      "robertabase &  mix_weat &  0.00015417001331457868 &  0.00021072992525229453 &  9396 &  8894 &  6710 \\\\ \n",
      "robertabase &  mix_all &  7.721378225833177e-06 &  9.313637732598159e-06 &  10520 &  10206 &  4274 \\\\ \n",
      "robertalarge &  original_Rall &  0.0019347595454967813 &  0.0023301372308227927 &  7105 &  13653 &  4242 \\\\ \n",
      "robertalarge &  original_Rweat &  0.0018032076409793807 &  0.0024627255408076767 &  5894 &  12411 &  6695 \\\\ \n",
      "robertalarge &  original_Rpro &  0.0009608211276997463 &  0.0014745566723446076 &  5235 &  11055 &  8710 \\\\ \n",
      "robertalarge &  N_pro &  0.0010052832913064048 &  0.0015429814760965202 &  5216 &  11072 &  8712 \\\\ \n",
      "robertalarge &  N_weat &  0.001702470466585073 &  0.0023255251701795885 &  6109 &  12193 &  6698 \\\\ \n",
      "robertalarge &  N_all &  0.001725592397772707 &  0.0020783258632903444 &  7045 &  13712 &  4243 \\\\ \n",
      "robertalarge &  mix_pro &  0.0002887483017554041 &  0.00044327341380934003 &  6679 &  9606 &  8715 \\\\ \n",
      "robertalarge &  mix_weat &  0.0002646826781181153 &  0.00036176627592547606 &  8071 &  10220 &  6709 \\\\ \n",
      "robertalarge &  mix_all &  0.0014529878857609583 &  0.0017502504164991787 &  6971 &  13783 &  4246 \\\\ \n"
     ]
    }
   ],
   "source": [
    "# specs = ['original_Rpro', 'mix_pro', 'original_Rall', 'mix_all', 'original_Rweat', 'N_pro', 'N_weat', 'mix_weat', 'N_all']\n",
    "specs = ['original_Rall', 'original_Rweat', 'original_Rpro', \"N_pro\", \"N_weat\", \"N_all\", \"mix_pro\", \"mix_weat\", \"mix_all\"]#, ]\n",
    "model_ids = [\"albertbase\", \"albertlarge\", \"bertbase\", \"bertlarge\", \"distbase\", \"robertabase\", \"robertalarge\"]\n",
    "\n",
    "\n",
    "# get_bias_bydict does basically the same as calc_bias_dict, but for singe specs instead of a whole model_id \n",
    "# get_bias_bydict takes returns single values per spec, while calc_bias_dict returns a dict wiht specs as keys and values as values\n",
    "def get_bias_bydict(dic, spec):\n",
    "    df = dic[spec]\n",
    "    df_no_zero = df.loc[(df != 0).all(axis=1)]\n",
    "    \n",
    "    bias = df.bias.mean()\n",
    "    zero_bias = df_no_zero.bias.mean()\n",
    "    \n",
    "    neg_count = 0\n",
    "    pos_count =0\n",
    "    zero_count = 0\n",
    "    for elem in df.bias.tolist():\n",
    "        if elem >0:\n",
    "            pos_count+=1\n",
    "        elif elem <0:\n",
    "            neg_count+=1\n",
    "        elif elem ==0:\n",
    "            zero_count+=1\n",
    "            \n",
    "    return bias, zero_bias, neg_count, pos_count, zero_count    \n",
    "    \n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    for spec in specs:\n",
    "        b, zb, neg, pos, zero = get_bias_bydict(dic, spec)\n",
    "        print(model, \"& \", spec,\"& \", b,\"& \", zb,\"& \", neg,\"& \", pos,\"& \", zero, \"\\\\\"\"\\\\ \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albertbase &  original_Rall & $ -0.0019 $ & $ -0.0023 $ &  9112 &  11645 &  4243 \\\\ \n",
      "albertbase &  original_Rweat & $ 0.0001 $ & $ 0.0002 $ &  7710 &  10600 &  6690 \\\\ \n",
      "albertbase &  original_Rpro & $ 0.0007 $ & $ 0.0011 $ &  5481 &  10811 &  8708 \\\\ \n",
      "albertbase &  N_pro & $ -0.0003 $ & $ -0.0004 $ &  9305 &  6986 &  8709 \\\\ \n",
      "albertbase &  N_weat & $ -0.0032 $ & $ -0.0044 $ &  10346 &  7942 &  6712 \\\\ \n",
      "albertbase &  N_all & $ 0.0008 $ & $ 0.0009 $ &  8979 &  11769 &  4252 \\\\ \n",
      "albertbase &  mix_pro & $ 0.0014 $ & $ 0.0021 $ &  7244 &  8968 &  8788 \\\\ \n",
      "albertbase &  mix_weat & $ -0.0024 $ & $ -0.0034 $ &  9426 &  8263 &  7311 \\\\ \n",
      "albertbase &  mix_all & $ -0.0012 $ & $ -0.0014 $ &  9481 &  11030 &  4489 \\\\ \n",
      "albertlarge &  original_Rall & $ 0.0114 $ & $ 0.0137 $ &  5095 &  15594 &  4311 \\\\ \n",
      "albertlarge &  original_Rweat & $ 0.0095 $ & $ 0.013 $ &  4058 &  14191 &  6751 \\\\ \n",
      "albertlarge &  original_Rpro & $ 0.0056 $ & $ 0.0086 $ &  2120 &  14075 &  8805 \\\\ \n",
      "albertlarge &  N_pro & $ 0.0022 $ & $ 0.0034 $ &  6407 &  9869 &  8724 \\\\ \n",
      "albertlarge &  N_weat & $ -0.0023 $ & $ -0.0032 $ &  9936 &  8373 &  6691 \\\\ \n",
      "albertlarge &  N_all & $ -0.0026 $ & $ -0.0032 $ &  12573 &  8180 &  4247 \\\\ \n",
      "albertlarge &  mix_pro & $ -0.0005 $ & $ -0.0008 $ &  10121 &  6136 &  8743 \\\\ \n",
      "albertlarge &  mix_weat & $ -0.0006 $ & $ -0.0009 $ &  9186 &  8998 &  6816 \\\\ \n",
      "albertlarge &  mix_all & $ 0.0028 $ & $ 0.0034 $ &  8777 &  11875 &  4348 \\\\ \n",
      "bertbase &  original_Rall & $ 0.0029 $ & $ 0.0035 $ &  5233 &  15527 &  4240 \\\\ \n",
      "bertbase &  original_Rweat & $ 0.0011 $ & $ 0.0015 $ &  6187 &  12128 &  6685 \\\\ \n",
      "bertbase &  original_Rpro & $ 0.0008 $ & $ 0.0013 $ &  5430 &  10874 &  8696 \\\\ \n",
      "bertbase &  N_pro & $ 0.002 $ & $ 0.0031 $ &  3234 &  13061 &  8705 \\\\ \n",
      "bertbase &  N_weat & $ 0.0015 $ & $ 0.002 $ &  6204 &  12098 &  6698 \\\\ \n",
      "bertbase &  N_all & $ 0.0034 $ & $ 0.0041 $ &  4319 &  16431 &  4250 \\\\ \n",
      "bertbase &  mix_pro & $ -0.0 $ & $ -0.0 $ &  7505 &  7794 &  9701 \\\\ \n",
      "bertbase &  mix_weat & $ -0.0001 $ & $ -0.0002 $ &  6421 &  7135 &  11444 \\\\ \n",
      "bertbase &  mix_all & $ 0.0003 $ & $ 0.0005 $ &  6838 &  9001 &  9161 \\\\ \n",
      "bertlarge &  original_Rall & $ 0.0007 $ & $ 0.0009 $ &  9128 &  11633 &  4239 \\\\ \n",
      "bertlarge &  original_Rweat & $ -0.0023 $ & $ -0.0032 $ &  10329 &  7986 &  6685 \\\\ \n",
      "bertlarge &  original_Rpro & $ -0.0011 $ & $ -0.0016 $ &  10287 &  6020 &  8693 \\\\ \n",
      "bertlarge &  N_pro & $ 0.003 $ & $ 0.0046 $ &  2697 &  13610 &  8693 \\\\ \n",
      "bertlarge &  N_weat & $ -0.0008 $ & $ -0.0011 $ &  10172 &  8142 &  6686 \\\\ \n",
      "bertlarge &  N_all & $ 0.0035 $ & $ 0.0042 $ &  7314 &  13443 &  4243 \\\\ \n",
      "bertlarge &  mix_pro & $ 0.0004 $ & $ 0.0011 $ &  4961 &  5228 &  14811 \\\\ \n",
      "bertlarge &  mix_weat & $ 0.0018 $ & $ 0.0034 $ &  4581 &  8195 &  12224 \\\\ \n",
      "bertlarge &  mix_all & $ 0.0012 $ & $ 0.0015 $ &  8848 &  10455 &  5697 \\\\ \n",
      "distbase &  original_Rall & $ 0.0013 $ & $ 0.0016 $ &  7817 &  12941 &  4242 \\\\ \n",
      "distbase &  original_Rweat & $ 0.0003 $ & $ 0.0004 $ &  8098 &  10214 &  6688 \\\\ \n",
      "distbase &  original_Rpro & $ 0.0006 $ & $ 0.0009 $ &  6085 &  10216 &  8699 \\\\ \n",
      "distbase &  N_pro & $ 0.0007 $ & $ 0.001 $ &  7116 &  9183 &  8701 \\\\ \n",
      "distbase &  N_weat & $ -0.0011 $ & $ -0.0015 $ &  10773 &  7532 &  6695 \\\\ \n",
      "distbase &  N_all & $ 0.0007 $ & $ 0.0008 $ &  10022 &  10734 &  4244 \\\\ \n",
      "distbase &  mix_pro & $ -0.0007 $ & $ -0.0012 $ &  6922 &  7309 &  10769 \\\\ \n",
      "distbase &  mix_weat & $ -0.0006 $ & $ -0.0008 $ &  8332 &  8428 &  8240 \\\\ \n",
      "distbase &  mix_all & $ -0.0003 $ & $ -0.0003 $ &  10080 &  10177 &  4743 \\\\ \n",
      "robertabase &  original_Rall & $ 0.0016 $ & $ 0.002 $ &  7165 &  13585 &  4250 \\\\ \n",
      "robertabase &  original_Rweat & $ 0.0011 $ & $ 0.0016 $ &  6470 &  11832 &  6698 \\\\ \n",
      "robertabase &  original_Rpro & $ 0.001 $ & $ 0.0016 $ &  5448 &  10840 &  8712 \\\\ \n",
      "robertabase &  N_pro & $ 0.0006 $ & $ 0.0009 $ &  6822 &  9472 &  8706 \\\\ \n",
      "robertabase &  N_weat & $ 0.0005 $ & $ 0.0007 $ &  7722 &  10581 &  6697 \\\\ \n",
      "robertabase &  N_all & $ 0.0008 $ & $ 0.001 $ &  9294 &  11464 &  4242 \\\\ \n",
      "robertabase &  mix_pro & $ -0.0001 $ & $ -0.0002 $ &  8682 &  7612 &  8706 \\\\ \n",
      "robertabase &  mix_weat & $ 0.0002 $ & $ 0.0002 $ &  9396 &  8894 &  6710 \\\\ \n",
      "robertabase &  mix_all & $ 0.0 $ & $ 0.0 $ &  10520 &  10206 &  4274 \\\\ \n",
      "robertalarge &  original_Rall & $ 0.0019 $ & $ 0.0023 $ &  7105 &  13653 &  4242 \\\\ \n",
      "robertalarge &  original_Rweat & $ 0.0018 $ & $ 0.0025 $ &  5894 &  12411 &  6695 \\\\ \n",
      "robertalarge &  original_Rpro & $ 0.001 $ & $ 0.0015 $ &  5235 &  11055 &  8710 \\\\ \n",
      "robertalarge &  N_pro & $ 0.001 $ & $ 0.0015 $ &  5216 &  11072 &  8712 \\\\ \n",
      "robertalarge &  N_weat & $ 0.0017 $ & $ 0.0023 $ &  6109 &  12193 &  6698 \\\\ \n",
      "robertalarge &  N_all & $ 0.0017 $ & $ 0.0021 $ &  7045 &  13712 &  4243 \\\\ \n",
      "robertalarge &  mix_pro & $ 0.0003 $ & $ 0.0004 $ &  6679 &  9606 &  8715 \\\\ \n",
      "robertalarge &  mix_weat & $ 0.0003 $ & $ 0.0004 $ &  8071 &  10220 &  6709 \\\\ \n",
      "robertalarge &  mix_all & $ 0.0015 $ & $ 0.0018 $ &  6971 &  13783 &  4246 \\\\ \n"
     ]
    }
   ],
   "source": [
    "    \n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    for spec in specs:\n",
    "        b, zb, neg, pos, zero = get_bias_bydict(dic, spec)\n",
    "        print(model, \"& \", spec,\"& $\", round(b,4 ),\"$ & $\", round(zb, 4),\"$ & \", neg,\"& \", pos,\"& \", zero, \"\\\\\"\"\\\\ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final complete bias table for supplementary results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albertbase &  original_Rpro &  0.0037 &  0.0011 &  0.0024 &  0.0007 &  5481 &  10811 &  8708 \\\\ \n",
      "albertbase &  N_pro &  0.0029 &  -0.0004 &  0.0019 &  -0.0003 &  9305 &  6986 &  8709 \\\\ \n",
      "albertbase &  mix_pro &  0.0054 &  0.0021 &  0.0035 &  0.0014 &  7244 &  8968 &  8788 \\\\ \n",
      "albertbase &  original_Rweat &  0.0093 &  0.0002 &  0.0068 &  0.0001 &  7710 &  10600 &  6690 \\\\ \n",
      "albertbase &  N_weat &  0.0082 &  -0.0044 &  0.006 &  -0.0032 &  10346 &  7942 &  6712 \\\\ \n",
      "albertbase &  mix_weat &  0.0131 &  -0.0034 &  0.0093 &  -0.0024 &  9426 &  8263 &  7311 \\\\ \n",
      "albertbase &  original_Rall &  0.0089 &  -0.0023 &  0.0074 &  -0.0019 &  9112 &  11645 &  4243 \\\\ \n",
      "albertbase &  N_all &  0.008 &  0.0009 &  0.0067 &  0.0008 &  8979 &  11769 &  4252 \\\\ \n",
      "albertbase &  mix_all &  0.0071 &  -0.0014 &  0.0058 &  -0.0012 &  9481 &  11030 &  4489 \\\\ \n",
      "albertlarge &  original_Rpro &  0.0086 &  0.0086 &  0.0056 &  0.0056 &  2120 &  14075 &  8805 \\\\ \n",
      "albertlarge &  N_pro &  0.0049 &  0.0034 &  0.0032 &  0.0022 &  6407 &  9869 &  8724 \\\\ \n",
      "albertlarge &  mix_pro &  0.0016 &  -0.0008 &  0.001 &  -0.0005 &  10121 &  6136 &  8743 \\\\ \n",
      "albertlarge &  original_Rweat &  0.0155 &  0.013 &  0.0113 &  0.0095 &  4058 &  14191 &  6751 \\\\ \n",
      "albertlarge &  N_weat &  0.0074 &  -0.0032 &  0.0054 &  -0.0023 &  9936 &  8373 &  6691 \\\\ \n",
      "albertlarge &  mix_weat &  0.0091 &  -0.0009 &  0.0066 &  -0.0006 &  9186 &  8998 &  6816 \\\\ \n",
      "albertlarge &  original_Rall &  0.0172 &  0.0137 &  0.0143 &  0.0114 &  5095 &  15594 &  4311 \\\\ \n",
      "albertlarge &  N_all &  0.0114 &  -0.0032 &  0.0095 &  -0.0026 &  12573 &  8180 &  4247 \\\\ \n",
      "albertlarge &  mix_all &  0.0101 &  0.0034 &  0.0084 &  0.0028 &  8777 &  11875 &  4348 \\\\ \n",
      "bertbase &  original_Rpro &  0.0025 &  0.0013 &  0.0016 &  0.0008 &  5430 &  10874 &  8696 \\\\ \n",
      "bertbase &  N_pro &  0.0036 &  0.0031 &  0.0024 &  0.002 &  3234 &  13061 &  8705 \\\\ \n",
      "bertbase &  mix_pro &  0.0023 &  -0.0 &  0.0014 &  -0.0 &  7505 &  7794 &  9701 \\\\ \n",
      "bertbase &  original_Rweat &  0.0037 &  0.0015 &  0.0027 &  0.0011 &  6187 &  12128 &  6685 \\\\ \n",
      "bertbase &  N_weat &  0.0038 &  0.002 &  0.0028 &  0.0015 &  6204 &  12098 &  6698 \\\\ \n",
      "bertbase &  mix_weat &  0.0027 &  -0.0002 &  0.0015 &  -0.0001 &  6421 &  7135 &  11444 \\\\ \n",
      "bertbase &  original_Rall &  0.0056 &  0.0035 &  0.0046 &  0.0029 &  5233 &  15527 &  4240 \\\\ \n",
      "bertbase &  N_all &  0.006 &  0.0041 &  0.0049 &  0.0034 &  4319 &  16431 &  4250 \\\\ \n",
      "bertbase &  mix_all &  0.0055 &  0.0005 &  0.0035 &  0.0003 &  6838 &  9001 &  9161 \\\\ \n",
      "bertlarge &  original_Rpro &  0.0031 &  -0.0016 &  0.0021 &  -0.0011 &  10287 &  6020 &  8693 \\\\ \n",
      "bertlarge &  N_pro &  0.005 &  0.0046 &  0.0032 &  0.003 &  2697 &  13610 &  8693 \\\\ \n",
      "bertlarge &  mix_pro &  0.0035 &  0.0011 &  0.0014 &  0.0004 &  4961 &  5228 &  14811 \\\\ \n",
      "bertlarge &  original_Rweat &  0.0069 &  -0.0032 &  0.0051 &  -0.0023 &  10329 &  7986 &  6685 \\\\ \n",
      "bertlarge &  N_weat &  0.0048 &  -0.0011 &  0.0035 &  -0.0008 &  10172 &  8142 &  6686 \\\\ \n",
      "bertlarge &  mix_weat &  0.0056 &  0.0034 &  0.0029 &  0.0018 &  4581 &  8195 &  12224 \\\\ \n",
      "bertlarge &  original_Rall &  0.0082 &  0.0009 &  0.0068 &  0.0007 &  9128 &  11633 &  4239 \\\\ \n",
      "bertlarge &  N_all &  0.0095 &  0.0042 &  0.0079 &  0.0035 &  7314 &  13443 &  4243 \\\\ \n",
      "bertlarge &  mix_all &  0.0101 &  0.0015 &  0.0078 &  0.0012 &  8848 &  10455 &  5697 \\\\ \n",
      "distbase &  original_Rpro &  0.0021 &  0.0009 &  0.0014 &  0.0006 &  6085 &  10216 &  8699 \\\\ \n",
      "distbase &  N_pro &  0.0022 &  0.001 &  0.0014 &  0.0007 &  7116 &  9183 &  8701 \\\\ \n",
      "distbase &  mix_pro &  0.0022 &  -0.0012 &  0.0012 &  -0.0007 &  6922 &  7309 &  10769 \\\\ \n",
      "distbase &  original_Rweat &  0.0035 &  0.0004 &  0.0026 &  0.0003 &  8098 &  10214 &  6688 \\\\ \n",
      "distbase &  N_weat &  0.0037 &  -0.0015 &  0.0027 &  -0.0011 &  10773 &  7532 &  6695 \\\\ \n",
      "distbase &  mix_weat &  0.0027 &  -0.0008 &  0.0018 &  -0.0006 &  8332 &  8428 &  8240 \\\\ \n",
      "distbase &  original_Rall &  0.0047 &  0.0016 &  0.0039 &  0.0013 &  7817 &  12941 &  4242 \\\\ \n",
      "distbase &  N_all &  0.0045 &  0.0008 &  0.0037 &  0.0007 &  10022 &  10734 &  4244 \\\\ \n",
      "distbase &  mix_all &  0.0052 &  -0.0003 &  0.0042 &  -0.0003 &  10080 &  10177 &  4743 \\\\ \n",
      "robertabase &  original_Rpro &  0.0024 &  0.0016 &  0.0015 &  0.001 &  5448 &  10840 &  8712 \\\\ \n",
      "robertabase &  N_pro &  0.0024 &  0.0009 &  0.0015 &  0.0006 &  6822 &  9472 &  8706 \\\\ \n",
      "robertabase &  mix_pro &  0.0021 &  -0.0002 &  0.0013 &  -0.0001 &  8682 &  7612 &  8706 \\\\ \n",
      "robertabase &  original_Rweat &  0.0031 &  0.0016 &  0.0023 &  0.0011 &  6470 &  11832 &  6698 \\\\ \n",
      "robertabase &  N_weat &  0.0028 &  0.0007 &  0.0021 &  0.0005 &  7722 &  10581 &  6697 \\\\ \n",
      "robertabase &  mix_weat &  0.0023 &  0.0002 &  0.0017 &  0.0002 &  9396 &  8894 &  6710 \\\\ \n",
      "robertabase &  original_Rall &  0.0036 &  0.002 &  0.003 &  0.0016 &  7165 &  13585 &  4250 \\\\ \n",
      "robertabase &  N_all &  0.0038 &  0.001 &  0.0032 &  0.0008 &  9294 &  11464 &  4242 \\\\ \n",
      "robertabase &  mix_all &  0.0027 &  0.0 &  0.0023 &  0.0 &  10520 &  10206 &  4274 \\\\ \n",
      "robertalarge &  original_Rpro &  0.0024 &  0.0015 &  0.0016 &  0.001 &  5235 &  11055 &  8710 \\\\ \n",
      "robertalarge &  N_pro &  0.0025 &  0.0015 &  0.0016 &  0.001 &  5216 &  11072 &  8712 \\\\ \n",
      "robertalarge &  mix_pro &  0.002 &  0.0004 &  0.0013 &  0.0003 &  6679 &  9606 &  8715 \\\\ \n",
      "robertalarge &  original_Rweat &  0.0039 &  0.0025 &  0.0029 &  0.0018 &  5894 &  12411 &  6695 \\\\ \n",
      "robertalarge &  N_weat &  0.0039 &  0.0023 &  0.0029 &  0.0017 &  6109 &  12193 &  6698 \\\\ \n",
      "robertalarge &  mix_weat &  0.0028 &  0.0004 &  0.0021 &  0.0003 &  8071 &  10220 &  6709 \\\\ \n",
      "robertalarge &  original_Rall &  0.0044 &  0.0023 &  0.0036 &  0.0019 &  7105 &  13653 &  4242 \\\\ \n",
      "robertalarge &  N_all &  0.0043 &  0.0021 &  0.0035 &  0.0017 &  7045 &  13712 &  4243 \\\\ \n",
      "robertalarge &  mix_all &  0.0041 &  0.0018 &  0.0034 &  0.0015 &  6971 &  13783 &  4246 \\\\ \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# final complete bias table for supplementary results\n",
    "\n",
    "for model in model_ids: # ['distbase', 'bertbase', 'bertlarge', 'robertabase', 'robertalarge', 'albertabase', 'albertalarge']: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    bias_dict = calc_bias_dict(dic) \n",
    "    for spec in ['original_Rpro', \"N_pro\", \"mix_pro\",\n",
    "                 'original_Rweat', \"N_weat\", \"mix_weat\",\n",
    "                 'original_Rall', \"N_all\", \"mix_all\"]:\n",
    "        overall_bias_total, overall_bias_abs, pos, neg, pos_n, neg_n, overall_bias_total_noZero, overall_bias_abs_noZero = bias_dict[spec]\n",
    "        b, zb, neg, pos, zero = get_bias_bydict(dic, spec)\n",
    "        assert(overall_bias_total==b)\n",
    "        assert(overall_bias_total_noZero==zb)\n",
    "        print(model, \"& \", spec,\"& \",\n",
    "              r(overall_bias_abs_noZero), \"& \", r(zb), \"& \",\n",
    "              r(overall_bias_abs), \"& \", r(b), \"& \",\n",
    "              neg,\"& \", pos,\"& \", zero, \"\\\\\"\"\\\\ \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final condensed bias table for paper results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>ss</th>\n",
       "      <td>original_Rpro</td>\n",
       "      <td>N_pro</td>\n",
       "      <td>mix_pro</td>\n",
       "      <td>original_Rweat</td>\n",
       "      <td>N_weat</td>\n",
       "      <td>mix_weat</td>\n",
       "      <td>original_Rall</td>\n",
       "      <td>N_all</td>\n",
       "      <td>mix_all</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albertbase_abs</th>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0029</td>\n",
       "      <td>0.0054</td>\n",
       "      <td>0.0093</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0131</td>\n",
       "      <td>0.0089</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.0071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albertbase_tot</th>\n",
       "      <td>0.0011</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>-0.0044</td>\n",
       "      <td>-0.0034</td>\n",
       "      <td>-0.0023</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albertlarge_abs</th>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0049</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0155</td>\n",
       "      <td>0.0074</td>\n",
       "      <td>0.0091</td>\n",
       "      <td>0.0172</td>\n",
       "      <td>0.0114</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>albertlarge_tot</th>\n",
       "      <td>0.0086</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.013</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0137</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>0.0034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertbase_abs</th>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.006</td>\n",
       "      <td>0.0055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertbase_tot</th>\n",
       "      <td>0.0013</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0041</td>\n",
       "      <td>0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertlarge_abs</th>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0069</td>\n",
       "      <td>0.0048</td>\n",
       "      <td>0.0056</td>\n",
       "      <td>0.0082</td>\n",
       "      <td>0.0095</td>\n",
       "      <td>0.0101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bertlarge_tot</th>\n",
       "      <td>-0.0016</td>\n",
       "      <td>0.0046</td>\n",
       "      <td>0.0011</td>\n",
       "      <td>-0.0032</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>0.0034</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.0042</td>\n",
       "      <td>0.0015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distbase_abs</th>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0022</td>\n",
       "      <td>0.0035</td>\n",
       "      <td>0.0037</td>\n",
       "      <td>0.0027</td>\n",
       "      <td>0.0047</td>\n",
       "      <td>0.0045</td>\n",
       "      <td>0.0052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>distbase_tot</th>\n",
       "      <td>0.0009</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.0012</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>-0.0015</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0008</td>\n",
       "      <td>-0.0003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertabase_abs</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0031</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0036</td>\n",
       "      <td>0.0038</td>\n",
       "      <td>0.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertabase_tot</th>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0009</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0016</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertalarge_abs</th>\n",
       "      <td>0.0024</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.002</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0039</td>\n",
       "      <td>0.0028</td>\n",
       "      <td>0.0044</td>\n",
       "      <td>0.0043</td>\n",
       "      <td>0.0041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>robertalarge_tot</th>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0015</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0025</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0004</td>\n",
       "      <td>0.0023</td>\n",
       "      <td>0.0021</td>\n",
       "      <td>0.0018</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              0       1        2               3       4  \\\n",
       "ss                original_Rpro   N_pro  mix_pro  original_Rweat  N_weat   \n",
       "albertbase_abs           0.0037  0.0029   0.0054          0.0093  0.0082   \n",
       "albertbase_tot           0.0011 -0.0004   0.0021          0.0002 -0.0044   \n",
       "albertlarge_abs          0.0086  0.0049   0.0016          0.0155  0.0074   \n",
       "albertlarge_tot          0.0086  0.0034  -0.0008           0.013 -0.0032   \n",
       "bertbase_abs             0.0025  0.0036   0.0023          0.0037  0.0038   \n",
       "bertbase_tot             0.0013  0.0031     -0.0          0.0015   0.002   \n",
       "bertlarge_abs            0.0031   0.005   0.0035          0.0069  0.0048   \n",
       "bertlarge_tot           -0.0016  0.0046   0.0011         -0.0032 -0.0011   \n",
       "distbase_abs             0.0021  0.0022   0.0022          0.0035  0.0037   \n",
       "distbase_tot             0.0009   0.001  -0.0012          0.0004 -0.0015   \n",
       "robertabase_abs          0.0024  0.0024   0.0021          0.0031  0.0028   \n",
       "robertabase_tot          0.0016  0.0009  -0.0002          0.0016  0.0007   \n",
       "robertalarge_abs         0.0024  0.0025    0.002          0.0039  0.0039   \n",
       "robertalarge_tot         0.0015  0.0015   0.0004          0.0025  0.0023   \n",
       "\n",
       "                         5              6       7        8  \n",
       "ss                mix_weat  original_Rall   N_all  mix_all  \n",
       "albertbase_abs      0.0131         0.0089   0.008   0.0071  \n",
       "albertbase_tot     -0.0034        -0.0023  0.0009  -0.0014  \n",
       "albertlarge_abs     0.0091         0.0172  0.0114   0.0101  \n",
       "albertlarge_tot    -0.0009         0.0137 -0.0032   0.0034  \n",
       "bertbase_abs        0.0027         0.0056   0.006   0.0055  \n",
       "bertbase_tot       -0.0002         0.0035  0.0041   0.0005  \n",
       "bertlarge_abs       0.0056         0.0082  0.0095   0.0101  \n",
       "bertlarge_tot       0.0034         0.0009  0.0042   0.0015  \n",
       "distbase_abs        0.0027         0.0047  0.0045   0.0052  \n",
       "distbase_tot       -0.0008         0.0016  0.0008  -0.0003  \n",
       "robertabase_abs     0.0023         0.0036  0.0038   0.0027  \n",
       "robertabase_tot     0.0002          0.002   0.001      0.0  \n",
       "robertalarge_abs    0.0028         0.0044  0.0043   0.0041  \n",
       "robertalarge_tot    0.0004         0.0023  0.0021   0.0018  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "special_spec =  ['original_Rpro', \"N_pro\", \"mix_pro\",\n",
    "                 'original_Rweat', \"N_weat\", \"mix_weat\",\n",
    "                 'original_Rall', \"N_all\", \"mix_all\"]\n",
    "\n",
    "res_dic = {}\n",
    "res_dic['ss'] = special_spec\n",
    "for m in model_ids:\n",
    "    res_dic[m+'_abs'] = []\n",
    "    res_dic[m+'_tot'] = []\n",
    "\n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    bias_dict = calc_bias_dict(dic) \n",
    "\n",
    "    for spec in special_spec:\n",
    "        overall_bias_total, overall_bias_abs, pos, neg, pos_n, neg_n, overall_bias_total_noZero, overall_bias_abs_noZero = bias_dict[spec]\n",
    "        res_dic[model+'_abs'].append(r(overall_bias_abs_noZero))\n",
    "        res_dic[model+'_tot'].append(r(overall_bias_total_noZero))\n",
    "        \n",
    "res = pd.DataFrame(res_dic)\n",
    "res = res.transpose()\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllllll}\n",
      "\\toprule\n",
      "{} &              0 &       1 &        2 &               3 &       4 &         5 &              6 &       7 &        8 \\\\\n",
      "\\midrule\n",
      "ss               &  original\\_Rpro &   N\\_pro &  mix\\_pro &  original\\_Rweat &  N\\_weat &  mix\\_weat &  original\\_Rall &   N\\_all &  mix\\_all \\\\\n",
      "albertbase\\_abs   &         0.0037 &  0.0029 &   0.0054 &          0.0093 &  0.0082 &    0.0131 &         0.0089 &   0.008 &   0.0071 \\\\\n",
      "albertbase\\_tot   &         0.0011 & -0.0004 &   0.0021 &          0.0002 & -0.0044 &   -0.0034 &        -0.0023 &  0.0009 &  -0.0014 \\\\\n",
      "albertlarge\\_abs  &         0.0086 &  0.0049 &   0.0016 &          0.0155 &  0.0074 &    0.0091 &         0.0172 &  0.0114 &   0.0101 \\\\\n",
      "albertlarge\\_tot  &         0.0086 &  0.0034 &  -0.0008 &           0.013 & -0.0032 &   -0.0009 &         0.0137 & -0.0032 &   0.0034 \\\\\n",
      "bertbase\\_abs     &         0.0025 &  0.0036 &   0.0023 &          0.0037 &  0.0038 &    0.0027 &         0.0056 &   0.006 &   0.0055 \\\\\n",
      "bertbase\\_tot     &         0.0013 &  0.0031 &     -0.0 &          0.0015 &   0.002 &   -0.0002 &         0.0035 &  0.0041 &   0.0005 \\\\\n",
      "bertlarge\\_abs    &         0.0031 &   0.005 &   0.0035 &          0.0069 &  0.0048 &    0.0056 &         0.0082 &  0.0095 &   0.0101 \\\\\n",
      "bertlarge\\_tot    &        -0.0016 &  0.0046 &   0.0011 &         -0.0032 & -0.0011 &    0.0034 &         0.0009 &  0.0042 &   0.0015 \\\\\n",
      "distbase\\_abs     &         0.0021 &  0.0022 &   0.0022 &          0.0035 &  0.0037 &    0.0027 &         0.0047 &  0.0045 &   0.0052 \\\\\n",
      "distbase\\_tot     &         0.0009 &   0.001 &  -0.0012 &          0.0004 & -0.0015 &   -0.0008 &         0.0016 &  0.0008 &  -0.0003 \\\\\n",
      "robertabase\\_abs  &         0.0024 &  0.0024 &   0.0021 &          0.0031 &  0.0028 &    0.0023 &         0.0036 &  0.0038 &   0.0027 \\\\\n",
      "robertabase\\_tot  &         0.0016 &  0.0009 &  -0.0002 &          0.0016 &  0.0007 &    0.0002 &          0.002 &   0.001 &      0.0 \\\\\n",
      "robertalarge\\_abs &         0.0024 &  0.0025 &    0.002 &          0.0039 &  0.0039 &    0.0028 &         0.0044 &  0.0043 &   0.0041 \\\\\n",
      "robertalarge\\_tot &         0.0015 &  0.0015 &   0.0004 &          0.0025 &  0.0023 &    0.0004 &         0.0023 &  0.0021 &   0.0018 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllllll}\n",
      "\\toprule\n",
      "{} &         4 &         2 &          3 &            5 &             6 &           0 &            1 \\\\\n",
      "\\midrule\n",
      "mods               &  distbase &  bertbase &  bertlarge &  robertabase &  robertalarge &  albertbase &  albertlarge \\\\\n",
      "original\\_Rpro\\_abs  &    0.0021 &    0.0025 &     0.0031 &       0.0024 &        0.0024 &      0.0037 &       0.0086 \\\\\n",
      "original\\_Rpro\\_tot  &    0.0009 &    0.0013 &    -0.0016 &       0.0016 &        0.0015 &      0.0011 &       0.0086 \\\\\n",
      "N\\_pro\\_abs          &    0.0022 &    0.0036 &      0.005 &       0.0024 &        0.0025 &      0.0029 &       0.0049 \\\\\n",
      "N\\_pro\\_tot          &     0.001 &    0.0031 &     0.0046 &       0.0009 &        0.0015 &     -0.0004 &       0.0034 \\\\\n",
      "mix\\_pro\\_abs        &    0.0022 &    0.0023 &     0.0035 &       0.0021 &         0.002 &      0.0054 &       0.0016 \\\\\n",
      "mix\\_pro\\_tot        &   -0.0012 &      -0.0 &     0.0011 &      -0.0002 &        0.0004 &      0.0021 &      -0.0008 \\\\\n",
      "original\\_Rweat\\_abs &    0.0035 &    0.0037 &     0.0069 &       0.0031 &        0.0039 &      0.0093 &       0.0155 \\\\\n",
      "original\\_Rweat\\_tot &    0.0004 &    0.0015 &    -0.0032 &       0.0016 &        0.0025 &      0.0002 &        0.013 \\\\\n",
      "N\\_weat\\_abs         &    0.0037 &    0.0038 &     0.0048 &       0.0028 &        0.0039 &      0.0082 &       0.0074 \\\\\n",
      "N\\_weat\\_tot         &   -0.0015 &     0.002 &    -0.0011 &       0.0007 &        0.0023 &     -0.0044 &      -0.0032 \\\\\n",
      "mix\\_weat\\_abs       &    0.0027 &    0.0027 &     0.0056 &       0.0023 &        0.0028 &      0.0131 &       0.0091 \\\\\n",
      "mix\\_weat\\_tot       &   -0.0008 &   -0.0002 &     0.0034 &       0.0002 &        0.0004 &     -0.0034 &      -0.0009 \\\\\n",
      "original\\_Rall\\_abs  &    0.0047 &    0.0056 &     0.0082 &       0.0036 &        0.0044 &      0.0089 &       0.0172 \\\\\n",
      "original\\_Rall\\_tot  &    0.0016 &    0.0035 &     0.0009 &        0.002 &        0.0023 &     -0.0023 &       0.0137 \\\\\n",
      "N\\_all\\_abs          &    0.0045 &     0.006 &     0.0095 &       0.0038 &        0.0043 &       0.008 &       0.0114 \\\\\n",
      "N\\_all\\_tot          &    0.0008 &    0.0041 &     0.0042 &        0.001 &        0.0021 &      0.0009 &      -0.0032 \\\\\n",
      "mix\\_all\\_abs        &    0.0052 &    0.0055 &     0.0101 &       0.0027 &        0.0041 &      0.0071 &       0.0101 \\\\\n",
      "mix\\_all\\_tot        &   -0.0003 &    0.0005 &     0.0015 &          0.0 &        0.0018 &     -0.0014 &       0.0034 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "special_spec =  ['original_Rpro', \"N_pro\", \"mix_pro\",\n",
    "                 'original_Rweat', \"N_weat\", \"mix_weat\",\n",
    "                 'original_Rall', \"N_all\", \"mix_all\"]\n",
    "\n",
    "res_dic = {'mods':model_ids}\n",
    "for spec in special_spec:\n",
    "    res_dic[spec+'_abs'] = []\n",
    "    res_dic[spec+'_tot'] = []\n",
    "\n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    bias_dict = calc_bias_dict(dic) \n",
    "\n",
    "    for spec in special_spec:\n",
    "        overall_bias_total, overall_bias_abs, pos, neg, pos_n, neg_n, overall_bias_total_noZero, overall_bias_abs_noZero = bias_dict[spec]\n",
    "        res_dic[spec+'_abs'].append(r(overall_bias_abs_noZero))\n",
    "        res_dic[spec+'_tot'].append(r(overall_bias_total_noZero))\n",
    "        \n",
    "res = pd.DataFrame(res_dic)\n",
    "res = res.transpose()\n",
    "\n",
    "print(res[[4, 2, 3, 5,6, 0, 1]].to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_albertbase = get_bias('IMDB', model_id_=\"albertbase\")\n",
    "dict_albertlarge = get_bias('IMDB', model_id_=\"albertlarge\")\n",
    "dict_bertbase = get_bias('IMDB', model_id_=\"bertbase\")\n",
    "dict_bertlarge = get_bias('IMDB', model_id_=\"bertlarge\")\n",
    "dict_distbase = get_bias('IMDB', model_id_=\"distbase\")\n",
    "dict_robertabase = get_bias('IMDB', model_id_=\"robertabase\")\n",
    "dict_robertalarge = get_bias('IMDB', model_id_=\"robertalarge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_114647/1269478893.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_no_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df_no_zero = df.loc[(df != 0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---  \n",
    "  \n",
    "---  \n",
    "  \n",
    "---  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# merge bias and accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load accuracies \n",
    "accs = pd.read_pickle('../res_results/IMDB_evaluation_df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size_dic = {\n",
    "    \"albertbase\": 16,\n",
    "    \"albertlarge\": 8,\n",
    "    \"bertbase\": 32,\n",
    "    \"bertlarge\": 16,\n",
    "    \"distbase\": 32,\n",
    "    \"robertabase\": 32,\n",
    "    \"robertalarge\": 16 \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 1 1\n",
      "2 2 2 2\n",
      "3 3 3 3\n",
      "4 4 4 4\n",
      "5 5 5 5\n",
      "6 6 6 6\n",
      "7 7 7 7\n",
      "8 8 8 8\n",
      "9 9 9 9\n",
      "10 10 10 10\n",
      "11 11 11 11\n",
      "12 12 12 12\n",
      "13 13 13 13\n",
      "14 14 14 14\n",
      "15 15 15 15\n",
      "16 16 16 16\n",
      "17 17 17 17\n",
      "18 18 18 18\n",
      "19 19 19 19\n",
      "20 20 20 20\n",
      "21 21 21 21\n",
      "22 22 22 22\n",
      "23 23 23 23\n",
      "24 24 24 24\n",
      "25 25 25 25\n",
      "26 26 26 26\n",
      "27 27 27 27\n",
      "28 28 28 28\n",
      "29 29 29 29\n",
      "30 30 30 30\n",
      "31 31 31 31\n",
      "32 32 32 32\n",
      "33 33 33 33\n",
      "34 34 34 34\n",
      "35 35 35 35\n",
      "36 36 36 36\n",
      "37 37 37 37\n",
      "38 38 38 38\n",
      "39 39 39 39\n",
      "40 40 40 40\n",
      "41 41 41 41\n",
      "42 42 42 42\n",
      "43 43 43 43\n",
      "44 44 44 44\n",
      "45 45 45 45\n",
      "46 46 46 46\n",
      "47 47 47 47\n",
      "48 48 48 48\n",
      "49 49 49 49\n",
      "50 50 50 50\n",
      "51 51 51 51\n",
      "52 52 52 52\n",
      "53 53 53 53\n",
      "54 54 54 54\n",
      "55 55 55 55\n",
      "56 56 56 56\n",
      "57 57 57 57\n",
      "58 58 58 58\n",
      "59 59 59 59\n",
      "60 60 60 60\n",
      "61 61 61 61\n",
      "62 62 62 62\n",
      "63 63 63 63\n"
     ]
    }
   ],
   "source": [
    "specs_2 = ['original',      'original',      'original',       \"N_pro\", \"N_weat\", \"N_all\", \"mix_pro\", \"mix_weat\", \"mix_all\"]\n",
    "specs_1 = ['original_Rpro', 'original_Rall', 'original_Rweat', 'N_pro', 'N_weat', 'N_all', 'mix_pro', 'mix_weat', 'mix_all']\n",
    "\n",
    "model_ids = [\"albertbase\", \"albertlarge\", \"bertbase\", \"bertlarge\", \"distbase\", \"robertabase\", \"robertalarge\"]\n",
    "\n",
    "model_l = []\n",
    "spec_l = []\n",
    "abs_b_l = []\n",
    "abs_zb_l = []\n",
    "b_l = []\n",
    "zb_l = []\n",
    "b_pos = []\n",
    "b_neg = []\n",
    "acc_l = []\n",
    "rec_l = []\n",
    "pre_l = []\n",
    "f1s_l = []\n",
    "bs_l = []\n",
    "lr_l = []\n",
    "epoch_l = []\n",
    "\n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    bias_dict = calc_bias_dict(dic) \n",
    "    \n",
    "    for i in range(len(specs_1)):\n",
    "        spec = specs_1[i]\n",
    "        spec_a = specs_2[i]\n",
    "        b, zb, neg, pos, zero = get_bias_bydict(dic, spec)\n",
    "        overall_bias_total, overall_bias_abs, pos, neg, pos_n, neg_n, overall_bias_total_noZero, overall_bias_abs_noZero = bias_dict[spec]\n",
    "        \n",
    "        b_pos.append(np.mean([x for x in dic[spec].bias.tolist() if x>0]))\n",
    "        b_neg.append(np.mean([x for x in dic[spec].bias.tolist() if x<0]))\n",
    "\n",
    "        df = accs\n",
    "        df = df.loc[(df['spec']== spec_a) & (df['model_id'] == model )]\n",
    "        # print(df)\n",
    "        if df.shape[0]==2:\n",
    "            #print('###')\n",
    "            df = df.loc[(df['test_data']== 'equal') & (df['model_id'] == model ),['accuracy','recall', 'precision', 'f1', 'test_data']]\n",
    "        assert(df.shape[0] ==1)\n",
    "        \n",
    "        model_l.append(model) \n",
    "        spec_l.append(spec) \n",
    "        assert(b == overall_bias_total )\n",
    "        assert(zb == overall_bias_total_noZero )\n",
    "        abs_b_l.append(overall_bias_abs)\n",
    "        abs_zb_l.append(overall_bias_abs_noZero) \n",
    "        b_l.append(b) \n",
    "        zb_l.append(zb) \n",
    "        acc_l.append(df.iloc[0]['accuracy'])\n",
    "        rec_l.append(df.iloc[0]['recall'])\n",
    "        pre_l.append(df.iloc[0]['precision'])\n",
    "        f1s_l.append(df.iloc[0]['f1'])\n",
    "        \n",
    "        bs_l.append(batch_size_dic[model])\n",
    "        for elem in IMDB_training_details:\n",
    "            #print(elem[1], model , elem[2],spec )\n",
    "            if elem[1]== model and elem[2]==spec_a: \n",
    "                lr = elem[3]\n",
    "                ep = elem[4]\n",
    "        lr_l.append(lr)\n",
    "        epoch_l.append(ep)\n",
    "\n",
    "        #print(model, \"& \", spec,\"& $\", round(b,4 ),\"$ & $\", round(zb, 4),\"$ & \", neg,\"& \", pos,\"& \", zero, \"\\\\\"\"\\\\ \")\n",
    "        print(len(acc_l), len(rec_l), len(pre_l), len(f1s_l))\n",
    "\n",
    "assert(len(model_l) == len(spec_l) == len(b_l) == len(zb_l) == len(b_pos) == len(b_neg) == len(acc_l) == len(rec_l) == len(pre_l) == len(f1s_l))\n",
    "acc_bias_df = pd.DataFrame({\n",
    "    \"model\": model_l,\n",
    "    \"spec\": spec_l,\n",
    "    \"abs_bias\": abs_b_l,\n",
    "    \"abs_bias_nz\": abs_zb_l,\n",
    "    \"bias\": b_l,\n",
    "    \"bias_nz\": zb_l,\n",
    "    \"bias_p\": b_pos,\n",
    "    \"bias_n\": b_neg,\n",
    "    \"accuracy\": acc_l,\n",
    "    \"recall\": rec_l,\n",
    "    \"precision\": pre_l,\n",
    "    \"f1\": f1s_l,\n",
    "    \"batch_size\": bs_l,\n",
    "    \"lr\": lr_l,\n",
    "    \"epoch\": epoch_l\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_bias</th>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_nz</th>\n",
       "      <th>bias_p</th>\n",
       "      <th>bias_n</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "      <td>63.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.003904</td>\n",
       "      <td>0.005260</td>\n",
       "      <td>0.000881</td>\n",
       "      <td>0.001198</td>\n",
       "      <td>0.005306</td>\n",
       "      <td>-0.004546</td>\n",
       "      <td>0.800241</td>\n",
       "      <td>0.776937</td>\n",
       "      <td>0.822896</td>\n",
       "      <td>0.796176</td>\n",
       "      <td>21.714286</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>11.047619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.002739</td>\n",
       "      <td>0.003364</td>\n",
       "      <td>0.002352</td>\n",
       "      <td>0.003108</td>\n",
       "      <td>0.003628</td>\n",
       "      <td>0.003387</td>\n",
       "      <td>0.031634</td>\n",
       "      <td>0.046971</td>\n",
       "      <td>0.058452</td>\n",
       "      <td>0.020779</td>\n",
       "      <td>9.359192</td>\n",
       "      <td>0.060050</td>\n",
       "      <td>4.308389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.001036</td>\n",
       "      <td>0.001593</td>\n",
       "      <td>-0.003244</td>\n",
       "      <td>-0.004435</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>-0.015527</td>\n",
       "      <td>0.692880</td>\n",
       "      <td>0.659840</td>\n",
       "      <td>0.630437</td>\n",
       "      <td>0.749443</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.001875</td>\n",
       "      <td>0.002791</td>\n",
       "      <td>-0.000188</td>\n",
       "      <td>-0.000269</td>\n",
       "      <td>0.002880</td>\n",
       "      <td>-0.005674</td>\n",
       "      <td>0.783710</td>\n",
       "      <td>0.748240</td>\n",
       "      <td>0.798586</td>\n",
       "      <td>0.780596</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.003013</td>\n",
       "      <td>0.003912</td>\n",
       "      <td>0.000692</td>\n",
       "      <td>0.000944</td>\n",
       "      <td>0.004370</td>\n",
       "      <td>-0.003238</td>\n",
       "      <td>0.814560</td>\n",
       "      <td>0.777600</td>\n",
       "      <td>0.839959</td>\n",
       "      <td>0.803509</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>11.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.005263</td>\n",
       "      <td>0.007027</td>\n",
       "      <td>0.001550</td>\n",
       "      <td>0.002045</td>\n",
       "      <td>0.006221</td>\n",
       "      <td>-0.002203</td>\n",
       "      <td>0.819640</td>\n",
       "      <td>0.789160</td>\n",
       "      <td>0.862380</td>\n",
       "      <td>0.811721</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>13.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.014267</td>\n",
       "      <td>0.017240</td>\n",
       "      <td>0.011371</td>\n",
       "      <td>0.013741</td>\n",
       "      <td>0.020551</td>\n",
       "      <td>-0.000033</td>\n",
       "      <td>0.835500</td>\n",
       "      <td>0.932240</td>\n",
       "      <td>0.882478</td>\n",
       "      <td>0.832061</td>\n",
       "      <td>32.000000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        abs_bias  abs_bias_nz       bias    bias_nz     bias_p     bias_n  \\\n",
       "count  63.000000    63.000000  63.000000  63.000000  63.000000  63.000000   \n",
       "mean    0.003904     0.005260   0.000881   0.001198   0.005306  -0.004546   \n",
       "std     0.002739     0.003364   0.002352   0.003108   0.003628   0.003387   \n",
       "min     0.001036     0.001593  -0.003244  -0.004435   0.000926  -0.015527   \n",
       "25%     0.001875     0.002791  -0.000188  -0.000269   0.002880  -0.005674   \n",
       "50%     0.003013     0.003912   0.000692   0.000944   0.004370  -0.003238   \n",
       "75%     0.005263     0.007027   0.001550   0.002045   0.006221  -0.002203   \n",
       "max     0.014267     0.017240   0.011371   0.013741   0.020551  -0.000033   \n",
       "\n",
       "        accuracy     recall  precision         f1  batch_size         lr  \\\n",
       "count  63.000000  63.000000  63.000000  63.000000   63.000000  63.000000   \n",
       "mean    0.800241   0.776937   0.822896   0.796176   21.714286   0.095238   \n",
       "std     0.031634   0.046971   0.058452   0.020779    9.359192   0.060050   \n",
       "min     0.692880   0.659840   0.630437   0.749443    8.000000   0.050000   \n",
       "25%     0.783710   0.748240   0.798586   0.780596   16.000000   0.050000   \n",
       "50%     0.814560   0.777600   0.839959   0.803509   16.000000   0.050000   \n",
       "75%     0.819640   0.789160   0.862380   0.811721   32.000000   0.100000   \n",
       "max     0.835500   0.932240   0.882478   0.832061   32.000000   0.200000   \n",
       "\n",
       "           epoch  \n",
       "count  63.000000  \n",
       "mean   11.047619  \n",
       "std     4.308389  \n",
       "min     4.000000  \n",
       "25%     8.000000  \n",
       "50%    11.000000  \n",
       "75%    13.000000  \n",
       "max    19.000000  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_bias_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_bias</th>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <th>bias</th>\n",
       "      <th>bias_nz</th>\n",
       "      <th>bias_p</th>\n",
       "      <th>bias_n</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_bias</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986988</td>\n",
       "      <td>0.391281</td>\n",
       "      <td>0.360378</td>\n",
       "      <td>0.925773</td>\n",
       "      <td>-0.747664</td>\n",
       "      <td>-0.453647</td>\n",
       "      <td>0.183805</td>\n",
       "      <td>-0.464667</td>\n",
       "      <td>-0.459802</td>\n",
       "      <td>-0.535054</td>\n",
       "      <td>0.312904</td>\n",
       "      <td>0.223250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <td>0.986988</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.392828</td>\n",
       "      <td>0.372960</td>\n",
       "      <td>0.937695</td>\n",
       "      <td>-0.739893</td>\n",
       "      <td>-0.480855</td>\n",
       "      <td>0.178438</td>\n",
       "      <td>-0.486080</td>\n",
       "      <td>-0.496982</td>\n",
       "      <td>-0.570574</td>\n",
       "      <td>0.384817</td>\n",
       "      <td>0.224320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias</th>\n",
       "      <td>0.391281</td>\n",
       "      <td>0.392828</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.993999</td>\n",
       "      <td>0.654125</td>\n",
       "      <td>0.289798</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>-0.244035</td>\n",
       "      <td>0.132373</td>\n",
       "      <td>-0.063398</td>\n",
       "      <td>-0.162859</td>\n",
       "      <td>0.262973</td>\n",
       "      <td>0.049358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_nz</th>\n",
       "      <td>0.360378</td>\n",
       "      <td>0.372960</td>\n",
       "      <td>0.993999</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.640023</td>\n",
       "      <td>0.321998</td>\n",
       "      <td>0.044056</td>\n",
       "      <td>-0.250922</td>\n",
       "      <td>0.126380</td>\n",
       "      <td>-0.085231</td>\n",
       "      <td>-0.174655</td>\n",
       "      <td>0.271259</td>\n",
       "      <td>0.048419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_p</th>\n",
       "      <td>0.925773</td>\n",
       "      <td>0.937695</td>\n",
       "      <td>0.654125</td>\n",
       "      <td>0.640023</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.487482</td>\n",
       "      <td>-0.359690</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>-0.340478</td>\n",
       "      <td>-0.421502</td>\n",
       "      <td>-0.531545</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>0.230960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_n</th>\n",
       "      <td>-0.747664</td>\n",
       "      <td>-0.739893</td>\n",
       "      <td>0.289798</td>\n",
       "      <td>0.321998</td>\n",
       "      <td>-0.487482</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.541940</td>\n",
       "      <td>-0.407249</td>\n",
       "      <td>0.601394</td>\n",
       "      <td>0.439259</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>-0.192896</td>\n",
       "      <td>-0.175183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>-0.453647</td>\n",
       "      <td>-0.480855</td>\n",
       "      <td>0.058979</td>\n",
       "      <td>0.044056</td>\n",
       "      <td>-0.359690</td>\n",
       "      <td>0.541940</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.561005</td>\n",
       "      <td>0.933316</td>\n",
       "      <td>0.886411</td>\n",
       "      <td>0.592581</td>\n",
       "      <td>-0.064883</td>\n",
       "      <td>0.072211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.183805</td>\n",
       "      <td>0.178438</td>\n",
       "      <td>-0.244035</td>\n",
       "      <td>-0.250922</td>\n",
       "      <td>0.048426</td>\n",
       "      <td>-0.407249</td>\n",
       "      <td>-0.561005</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.778842</td>\n",
       "      <td>-0.116956</td>\n",
       "      <td>-0.029757</td>\n",
       "      <td>0.115034</td>\n",
       "      <td>0.077771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>-0.464667</td>\n",
       "      <td>-0.486080</td>\n",
       "      <td>0.132373</td>\n",
       "      <td>0.126380</td>\n",
       "      <td>-0.340478</td>\n",
       "      <td>0.601394</td>\n",
       "      <td>0.933316</td>\n",
       "      <td>-0.778842</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.694691</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>-0.133557</td>\n",
       "      <td>-0.036355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>-0.459802</td>\n",
       "      <td>-0.496982</td>\n",
       "      <td>-0.063398</td>\n",
       "      <td>-0.085231</td>\n",
       "      <td>-0.421502</td>\n",
       "      <td>0.439259</td>\n",
       "      <td>0.886411</td>\n",
       "      <td>-0.116956</td>\n",
       "      <td>0.694691</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.706392</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>0.111774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <td>-0.535054</td>\n",
       "      <td>-0.570574</td>\n",
       "      <td>-0.162859</td>\n",
       "      <td>-0.174655</td>\n",
       "      <td>-0.531545</td>\n",
       "      <td>0.407143</td>\n",
       "      <td>0.592581</td>\n",
       "      <td>-0.029757</td>\n",
       "      <td>0.495498</td>\n",
       "      <td>0.706392</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.077076</td>\n",
       "      <td>-0.099656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.312904</td>\n",
       "      <td>0.384817</td>\n",
       "      <td>0.262973</td>\n",
       "      <td>0.271259</td>\n",
       "      <td>0.401761</td>\n",
       "      <td>-0.192896</td>\n",
       "      <td>-0.064883</td>\n",
       "      <td>0.115034</td>\n",
       "      <td>-0.133557</td>\n",
       "      <td>-0.028871</td>\n",
       "      <td>-0.077076</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.250259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>0.223250</td>\n",
       "      <td>0.224320</td>\n",
       "      <td>0.049358</td>\n",
       "      <td>0.048419</td>\n",
       "      <td>0.230960</td>\n",
       "      <td>-0.175183</td>\n",
       "      <td>0.072211</td>\n",
       "      <td>0.077771</td>\n",
       "      <td>-0.036355</td>\n",
       "      <td>0.111774</td>\n",
       "      <td>-0.099656</td>\n",
       "      <td>0.250259</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             abs_bias  abs_bias_nz      bias   bias_nz    bias_p    bias_n  \\\n",
       "abs_bias     1.000000     0.986988  0.391281  0.360378  0.925773 -0.747664   \n",
       "abs_bias_nz  0.986988     1.000000  0.392828  0.372960  0.937695 -0.739893   \n",
       "bias         0.391281     0.392828  1.000000  0.993999  0.654125  0.289798   \n",
       "bias_nz      0.360378     0.372960  0.993999  1.000000  0.640023  0.321998   \n",
       "bias_p       0.925773     0.937695  0.654125  0.640023  1.000000 -0.487482   \n",
       "bias_n      -0.747664    -0.739893  0.289798  0.321998 -0.487482  1.000000   \n",
       "accuracy    -0.453647    -0.480855  0.058979  0.044056 -0.359690  0.541940   \n",
       "recall       0.183805     0.178438 -0.244035 -0.250922  0.048426 -0.407249   \n",
       "precision   -0.464667    -0.486080  0.132373  0.126380 -0.340478  0.601394   \n",
       "f1          -0.459802    -0.496982 -0.063398 -0.085231 -0.421502  0.439259   \n",
       "batch_size  -0.535054    -0.570574 -0.162859 -0.174655 -0.531545  0.407143   \n",
       "lr           0.312904     0.384817  0.262973  0.271259  0.401761 -0.192896   \n",
       "epoch        0.223250     0.224320  0.049358  0.048419  0.230960 -0.175183   \n",
       "\n",
       "             accuracy    recall  precision        f1  batch_size        lr  \\\n",
       "abs_bias    -0.453647  0.183805  -0.464667 -0.459802   -0.535054  0.312904   \n",
       "abs_bias_nz -0.480855  0.178438  -0.486080 -0.496982   -0.570574  0.384817   \n",
       "bias         0.058979 -0.244035   0.132373 -0.063398   -0.162859  0.262973   \n",
       "bias_nz      0.044056 -0.250922   0.126380 -0.085231   -0.174655  0.271259   \n",
       "bias_p      -0.359690  0.048426  -0.340478 -0.421502   -0.531545  0.401761   \n",
       "bias_n       0.541940 -0.407249   0.601394  0.439259    0.407143 -0.192896   \n",
       "accuracy     1.000000 -0.561005   0.933316  0.886411    0.592581 -0.064883   \n",
       "recall      -0.561005  1.000000  -0.778842 -0.116956   -0.029757  0.115034   \n",
       "precision    0.933316 -0.778842   1.000000  0.694691    0.495498 -0.133557   \n",
       "f1           0.886411 -0.116956   0.694691  1.000000    0.706392 -0.028871   \n",
       "batch_size   0.592581 -0.029757   0.495498  0.706392    1.000000 -0.077076   \n",
       "lr          -0.064883  0.115034  -0.133557 -0.028871   -0.077076  1.000000   \n",
       "epoch        0.072211  0.077771  -0.036355  0.111774   -0.099656  0.250259   \n",
       "\n",
       "                epoch  \n",
       "abs_bias     0.223250  \n",
       "abs_bias_nz  0.224320  \n",
       "bias         0.049358  \n",
       "bias_nz      0.048419  \n",
       "bias_p       0.230960  \n",
       "bias_n      -0.175183  \n",
       "accuracy     0.072211  \n",
       "recall       0.077771  \n",
       "precision   -0.036355  \n",
       "f1           0.111774  \n",
       "batch_size  -0.099656  \n",
       "lr           0.250259  \n",
       "epoch        1.000000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_bias_df.corr(method=\"pearson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_starlets(val):\n",
    "    if val < 0.001:\n",
    "        return '***'\n",
    "    elif val< 0.01:\n",
    "        return '**'\n",
    "    elif val < 0.05:\n",
    "        return '*'\n",
    "    else:\n",
    "        return 'ns'\n",
    "    \n",
    "from scipy.stats import pearsonr\n",
    "def calculate_pvalues(df, starlets=False):\n",
    "    df = df.dropna()._get_numeric_data()\n",
    "    dfcols = pd.DataFrame(columns=df.columns)\n",
    "    pvalues = dfcols.transpose().join(dfcols, how='outer')\n",
    "    for r in df.columns:\n",
    "        for c in df.columns:\n",
    "            val = round(pearsonr(df[r], df[c])[1], 5)\n",
    "            if starlets: \n",
    "                pvalues[r][c] = get_starlets(val)\n",
    "            else:\n",
    "                pvalues[r][c] = val\n",
    "    return pvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['model', 'spec', 'abs_bias', 'abs_bias_nz', 'bias', 'bias_nz', 'bias_p',\n",
      "       'bias_n', 'accuracy', 'recall', 'precision', 'f1', 'batch_size', 'lr',\n",
      "       'epoch'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(acc_bias_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrr}\n",
      "\\toprule\n",
      "{} &  abs\\_bias\\_nz &   bias\\_nz &  accuracy &        f1 &  batch\\_size &        lr &     epoch \\\\\n",
      "\\midrule\n",
      "abs\\_bias\\_nz &     1.000000 &  0.372960 & -0.480855 & -0.496982 &   -0.570574 &  0.384817 &  0.224320 \\\\\n",
      "bias\\_nz     &     0.372960 &  1.000000 &  0.044056 & -0.085231 &   -0.174655 &  0.271259 &  0.048419 \\\\\n",
      "accuracy    &    -0.480855 &  0.044056 &  1.000000 &  0.886411 &    0.592581 & -0.064883 &  0.072211 \\\\\n",
      "f1          &    -0.496982 & -0.085231 &  0.886411 &  1.000000 &    0.706392 & -0.028871 &  0.111774 \\\\\n",
      "batch\\_size  &    -0.570574 & -0.174655 &  0.592581 &  0.706392 &    1.000000 & -0.077076 & -0.099656 \\\\\n",
      "lr          &     0.384817 &  0.271259 & -0.064883 & -0.028871 &   -0.077076 &  1.000000 &  0.250259 \\\\\n",
      "epoch       &     0.224320 &  0.048419 &  0.072211 &  0.111774 &   -0.099656 &  0.250259 &  1.000000 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bar = acc_bias_df[[\n",
    "    'model', \n",
    "    'spec', \n",
    "   # \"abs_bias\",\n",
    "    \"abs_bias_nz\",\n",
    "   # 'bias', \n",
    "    'bias_nz', \n",
    "   # 'bias_p', \n",
    "   # 'bias_n', \n",
    "    'accuracy',\n",
    "    #'recall', \n",
    "   # 'precision', \n",
    "    'f1', \n",
    "    'batch_size', \n",
    "    'lr',\n",
    "    'epoch']]\n",
    "print(bar.corr(method=\"pearson\").to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <th>bias_nz</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00261</td>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.07716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_nz</th>\n",
       "      <td>0.00261</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.73171</td>\n",
       "      <td>0.50659</td>\n",
       "      <td>0.17098</td>\n",
       "      <td>0.03152</td>\n",
       "      <td>0.70629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.00007</td>\n",
       "      <td>0.73171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.61341</td>\n",
       "      <td>0.57383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>0.00003</td>\n",
       "      <td>0.50659</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.82228</td>\n",
       "      <td>0.38313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.17098</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.54823</td>\n",
       "      <td>0.4371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>0.00185</td>\n",
       "      <td>0.03152</td>\n",
       "      <td>0.61341</td>\n",
       "      <td>0.82228</td>\n",
       "      <td>0.54823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.04791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>0.07716</td>\n",
       "      <td>0.70629</td>\n",
       "      <td>0.57383</td>\n",
       "      <td>0.38313</td>\n",
       "      <td>0.4371</td>\n",
       "      <td>0.04791</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            abs_bias_nz  bias_nz accuracy       f1 batch_size       lr  \\\n",
       "abs_bias_nz         0.0  0.00261  0.00007  0.00003        0.0  0.00185   \n",
       "bias_nz         0.00261      0.0  0.73171  0.50659    0.17098  0.03152   \n",
       "accuracy        0.00007  0.73171      0.0      0.0        0.0  0.61341   \n",
       "f1              0.00003  0.50659      0.0      0.0        0.0  0.82228   \n",
       "batch_size          0.0  0.17098      0.0      0.0        0.0  0.54823   \n",
       "lr              0.00185  0.03152  0.61341  0.82228    0.54823      0.0   \n",
       "epoch           0.07716  0.70629  0.57383  0.38313     0.4371  0.04791   \n",
       "\n",
       "               epoch  \n",
       "abs_bias_nz  0.07716  \n",
       "bias_nz      0.70629  \n",
       "accuracy     0.57383  \n",
       "f1           0.38313  \n",
       "batch_size    0.4371  \n",
       "lr           0.04791  \n",
       "epoch            0.0  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pvalues(bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <th>bias_nz</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>lr</th>\n",
       "      <th>epoch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>abs_bias_nz</th>\n",
       "      <td>***</td>\n",
       "      <td>**</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>**</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bias_nz</th>\n",
       "      <td>**</td>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>*</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1</th>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>batch_size</th>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>***</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lr</th>\n",
       "      <td>**</td>\n",
       "      <td>*</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>***</td>\n",
       "      <td>*</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>epoch</th>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>ns</td>\n",
       "      <td>*</td>\n",
       "      <td>***</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            abs_bias_nz bias_nz accuracy   f1 batch_size   lr epoch\n",
       "abs_bias_nz         ***      **      ***  ***        ***   **    ns\n",
       "bias_nz              **     ***       ns   ns         ns    *    ns\n",
       "accuracy            ***      ns      ***  ***        ***   ns    ns\n",
       "f1                  ***      ns      ***  ***        ***   ns    ns\n",
       "batch_size          ***      ns      ***  ***        ***   ns    ns\n",
       "lr                   **       *       ns   ns         ns  ***     *\n",
       "epoch                ns      ns       ns   ns         ns    *   ***"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calculate_pvalues(bar, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='b'>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo = acc_bias_df.copy()\n",
    "foo = foo.sort_values(by='b')\n",
    "foo.plot(x=\"b\", y=\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'text_pro_M', 'text_pro_F', 'pos_prob_m', 'pos_prob_f', 'bias'], dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic = get_bias('IMDB', model_id_ = model)\n",
    "dic['N_pro'].columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stats(df):\n",
    "    bias_twoSided = scipy.stats.wilcoxon(df['delta_rat'].tolist(), alternative=\"two-sided\", correction=True).pvalue\n",
    "    bias_oneSided = scipy.stats.wilcoxon(df['delta_rat'].tolist(), alternative=\"greater\", correction=True).pvalue\n",
    "    effect_size = [df[df['delta_rat']<0].shape[0] , df[df['delta_rat']==0].shape[0] , df[df['delta_rat']>0].shape[0]]\n",
    "#    print('sample relation to 0:    ', effect_size)\n",
    "#    print('pvalue two sided:     ', bias_twoSided)\n",
    "#    print('pvalue pos bias:      ',  bias_oneSided)\n",
    "#    print('mean: ', df['delta_rat'].mean(),'std: ', df['delta_rat'].std())\n",
    "   \n",
    "    bias_twoSided_abs = scipy.stats.wilcoxon(df['bias_abs'].tolist(), alternative=\"two-sided\", correction=True).pvalue\n",
    "    bias_oneSided_abs = scipy.stats.wilcoxon(df['bias_abs'].tolist(), alternative=\"greater\", correction=True).pvalue\n",
    "    effect_size_abs = [df[df['bias_abs']<0].shape[0] , df[df['bias_abs']==0].shape[0] , df[df['bias_abs']>0].shape[0]]\n",
    "#    print('sample relation to 0:    ', effect_size_abs)\n",
    "#    print('pvalue two sided:     ', bias_twoSided_abs)\n",
    "#    print('pvalue pos bias:      ',  bias_oneSided_abs)\n",
    "#    print('mean: ', df['bias_abs'].mean(),'std: ', df['bias_abs'].std())\n",
    "    \n",
    "    print('[',df['delta_rat'].mean(),', ', bias_twoSided, ', ', bias_oneSided, ', ', df['delta_rat'].std(),', ', effect_size, '], ')    \n",
    "    print('[',df['bias_abs'].mean(),', ', bias_twoSided_abs,', ', bias_oneSided_abs,', ', df['bias_abs'].std(),', ', effect_size_abs, '], ')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "from scipy.stats import ranksums, wilcoxon\n",
    "\n",
    "# You should use the signed rank test when the data are paired.\n",
    "# You should use the rank-sum test when the data are not paired.\n",
    "\n",
    "# You'll find many definitions of pairing, but at heart the criterion is something that makes pairs of values at least somewhat positively dependent, while unpaired values are not dependent. Often the dependence-pairing occurs because they're observations on the same unit (repeated measures), but it doesn't have to be on the same unit, just in some way tending to be associated (while measuring the same kind of thing), to be considered as 'paired'.\n",
    "\n",
    "\n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    for spec in specs:\n",
    "        m = dic[spec]['pos_prob_m'].tolist()\n",
    "        f = dic[spec]['pos_prob_f'].tolist()\n",
    "        print('####', model, spec, )\n",
    "       # print(ranksums(m,f))\n",
    "        rs = ranksums(f,m)\n",
    "        s = 'X'\n",
    "        if rs[1] < 0.001:\n",
    "            s = '***'\n",
    "        elif rs[1] < 0.01:\n",
    "            s = '**'\n",
    "        elif rs[1] < 0.05:\n",
    "            s = '*'\n",
    "        print(rs, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albertlarge mix_weat #### WilcoxonResult(statistic=82160517.0, pvalue=0.4725537616112815) X\n",
      "bertbase mix_pro #### WilcoxonResult(statistic=57645635.0, pvalue=0.1099774631247071) X\n",
      "distbase mix_all #### WilcoxonResult(statistic=100953063.5, pvalue=0.048991050176962585) *\n",
      "robertabase mix_weat #### WilcoxonResult(statistic=83453375.0, pvalue=0.7985806099014158) X\n",
      "robertabase mix_all #### WilcoxonResult(statistic=107260133.5, pvalue=0.8738001567421141) X\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ranksums, wilcoxon\n",
    "\n",
    "def stats(x,y,a=\"two-sided\"):\n",
    "        rs = wilcoxon(x,y,alternative=a)\n",
    "        s = 'X'\n",
    "        if rs[1] < 0.001:\n",
    "            s = '***'\n",
    "        elif rs[1] < 0.01:\n",
    "            s = '**'\n",
    "        elif rs[1] < 0.05:\n",
    "            s = '*'\n",
    "        return rs, s\n",
    "\n",
    "\n",
    "\n",
    "for model in model_ids: \n",
    "    dic = get_bias('IMDB', model_id_ = model)\n",
    "    for spec in specs:\n",
    "        m = dic[spec]['pos_prob_m'].tolist()\n",
    "        f = dic[spec]['pos_prob_f'].tolist()\n",
    "        rs, s = stats(m,f)\n",
    "        if rs[1]>0.001:\n",
    "            print( model, spec,'####', rs, s )\n",
    "        #print(stats(m,f, 'greater'))\n",
    "        #print(stats(m,f, 'less'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
