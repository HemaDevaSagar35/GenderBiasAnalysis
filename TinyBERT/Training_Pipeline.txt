#
###
The following are the steps to be followed while training TinyBERT for a particular task
1---> Download the custom data required for the task
2---> Unzip that custom data folder in data/glue_data
3---> Once the data folder is unzipped in data/glue_data, convert the data files in the folder to .tsv files
4---> download glove embeddings from here: https://nlp.stanford.edu/data/glove.6B.zip
5---> unzip glove embeddings in embeddings/ directory
6---> Incorporate respective values for the custom dataset in self.augment_ids and self.filter_flags in AugmentProcessor(object) and in default_params in main()
6---> Then run the data augmention on the dataset using the following command:
      python data_augmentation.py --pretrained_bert_model ${BERT_BASE_DIR}$ --glove_embs ${GLOVE_EMB}$ --glue_dir ${GLUE_DIR}$ --task_name ${TASK_NAME}$
7---> Step will save a train_aug.tsv file in the custom data folder
8---> For finetuning Bert:
      python FT_Bert_Classification.py --data_dir ../data/glue_data/MLMA --pre_trained_bert bert-base-uncased 
                                       --task_name MLMA --do_lower_case --learning_rate 0.01 --output_dir output_models
   
9---> Hyperparameters used for finetunine are : sequence length = 120 with right padding, batch_size=64, 
      learning_rate = 0.00002, wegith decay = 0.01, warmup = 10%, epoch = 30 and epoch 5 early stopping. For
      MLMA used weighted cross-entropy classification loss.




https://dl.acm.org/doi/pdf/10.1145/3278721.3278729